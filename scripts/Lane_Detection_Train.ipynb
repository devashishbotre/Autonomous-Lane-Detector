{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 2840219,
          "sourceType": "datasetVersion",
          "datasetId": 1724942
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Lane Detection Train",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devashishbotre/Autonomous-Lane-Detector/blob/main/scripts/Lane_Detection_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "manideep1108_tusimple_path = kagglehub.dataset_download('manideep1108/tusimple')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "C5Ko8bN9Y48c"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from scipy.interpolate import CubicSpline\n",
        "from sklearn.cluster import DBSCAN\n",
        "import torchvision.models as models\n",
        "from model_module import RESA, Decoder, LaneNet\n",
        "\n",
        "# Constants\n",
        "IMG_HEIGHT = 720\n",
        "IMG_WIDTH = 1280\n",
        "NUM_CLASSES = 2"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T12:50:16.150068Z",
          "iopub.execute_input": "2025-04-09T12:50:16.150484Z",
          "iopub.status.idle": "2025-04-09T12:50:16.156165Z",
          "shell.execute_reply.started": "2025-04-09T12:50:16.150451Z",
          "shell.execute_reply": "2025-04-09T12:50:16.155138Z"
        },
        "id": "z4g6EP2MY48g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class TuSimpleDataset(Dataset):\n",
        "    def __init__(self, json_files, img_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.annotations = []\n",
        "        total_annotations = 0\n",
        "        for json_file in json_files:\n",
        "            if not os.path.exists(json_file):\n",
        "                print(f\"Warning: JSON file not found: {json_file}\")\n",
        "                continue\n",
        "            with open(json_file, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "                total_annotations += len(lines)\n",
        "                for line in lines:\n",
        "                    ann = json.loads(line)\n",
        "                    img_path = os.path.join(self.img_dir, ann['raw_file'])\n",
        "                    if os.path.exists(img_path):\n",
        "                        self.annotations.append(ann)\n",
        "                    else:\n",
        "                        print(f\"Warning: Image not found: {img_path}\")\n",
        "        print(f\"Total annotations in JSON: {total_annotations}\")\n",
        "        print(f\"Valid images found: {len(self.annotations)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ann = self.annotations[idx]\n",
        "        img_path = os.path.join(self.img_dir, ann['raw_file'])\n",
        "\n",
        "        # Load and resize image\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            print(f\"Error: Failed to load image: {img_path}\")\n",
        "            image = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\n",
        "        image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = np.zeros((IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)\n",
        "        for lane in ann['lanes']:\n",
        "            points = [(x, y) for x, y in zip(lane, ann['h_samples']) if x != -2 and 0 <= y < IMG_HEIGHT and 0 <= x < IMG_WIDTH]\n",
        "            if len(points) > 1:\n",
        "                points = np.array(points, dtype=np.int32)\n",
        "                cv2.polylines(mask, [points], False, 1, 1)\n",
        "\n",
        "        image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
        "        mask = torch.from_numpy(mask).long()\n",
        "\n",
        "        return image, mask, ann\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, masks, anns = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    masks = torch.stack(masks, 0)\n",
        "    return images, masks, anns"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T12:23:30.290987Z",
          "iopub.execute_input": "2025-04-09T12:23:30.291285Z",
          "iopub.status.idle": "2025-04-09T12:23:30.301751Z",
          "shell.execute_reply.started": "2025-04-09T12:23:30.291263Z",
          "shell.execute_reply": "2025-04-09T12:23:30.300796Z"
        },
        "id": "HqOn-Lc7Y48g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    batch_size = 4\n",
        "    num_epochs = 20\n",
        "    learning_rate = 0.0002\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = LaneNet(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "    weights = torch.tensor([1.0, 25.0]).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "    tv_weight = 0.3\n",
        "    narrowness_weight = 0.5\n",
        "    edge_weight = 0.2\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    data_dir = '/kaggle/input/tusimple/TUSimple/train_set'\n",
        "    json_files = [os.path.join(data_dir, f) for f in ['label_data_0313.json',\n",
        "                                                     'label_data_0531.json',\n",
        "                                                     'label_data_0601.json']]\n",
        "    img_dir = data_dir\n",
        "    train_dataset = TuSimpleDataset(json_files, img_dir)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, masks, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            seg_loss = criterion(outputs, masks)\n",
        "\n",
        "            def total_variation_loss(x):\n",
        "                batch_size = x.size(0)\n",
        "                h_x = x.size(2)\n",
        "                w_x = x.size(3)\n",
        "                dh = torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]).sum() / (batch_size * h_x * w_x)\n",
        "                dw = torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]).sum() / (batch_size * h_x * w_x)\n",
        "                return (dh + dw) / 2\n",
        "\n",
        "            def narrowness_penalty(x):\n",
        "                probs = torch.softmax(x, dim=1)\n",
        "                lane_prob = probs[:, 1, :, :]\n",
        "                kernel = torch.ones(1, 1, 3, 3).to(device) / 9\n",
        "                blurred = F.conv2d(lane_prob.unsqueeze(1), kernel, padding=1)\n",
        "\n",
        "                blurred = torch.clamp(blurred, min=1e-6, max=1.0 - 1e-6)\n",
        "                lane_prob = torch.clamp(lane_prob, min=1e-6, max=1.0 - 1e-6)\n",
        "\n",
        "                width_penalty = torch.pow(blurred.squeeze(1), 2).mean()\n",
        "                variance = torch.pow(lane_prob - blurred.squeeze(1), 2).mean()\n",
        "                return width_penalty + variance\n",
        "\n",
        "            def edge_enhancement_loss(x):\n",
        "                probs = torch.softmax(x, dim=1)\n",
        "                lane_prob = probs[:, 1, :, :]\n",
        "\n",
        "                sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).to(device)\n",
        "                sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).to(device)\n",
        "\n",
        "                sobel_x = sobel_x.view(1, 1, 3, 3)\n",
        "                sobel_y = sobel_y.view(1, 1, 3, 3)\n",
        "\n",
        "                edge_x = F.conv2d(lane_prob.unsqueeze(1), sobel_x, padding=1)\n",
        "                edge_y = F.conv2d(lane_prob.unsqueeze(1), sobel_y, padding=1)\n",
        "\n",
        "                edges = torch.sqrt(edge_x.pow(2) + edge_y.pow(2) + 1e-8)\n",
        "                return -torch.clamp(edges.mean(), max=10.0)\n",
        "\n",
        "            tv_loss = total_variation_loss(torch.softmax(outputs, dim=1))\n",
        "            narrowness_loss = narrowness_penalty(outputs)\n",
        "            edge_loss = edge_enhancement_loss(outputs)\n",
        "\n",
        "            total_loss = seg_loss + tv_weight * tv_loss + narrowness_weight * narrowness_loss + edge_weight * edge_loss\n",
        "\n",
        "            if torch.isnan(total_loss):\n",
        "                print(f\"NaN detected: seg_loss={seg_loss.item()}, tv_loss={tv_loss.item()}, \"\n",
        "                      f\"narrowness_loss={narrowness_loss.item()}, edge_loss={edge_loss.item()}\")\n",
        "                break\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            running_loss += total_loss.item()\n",
        "\n",
        "        if torch.isnan(total_loss):\n",
        "            break\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            torch.save(model.state_dict(), f\"lane_model_epoch_{epoch+1}.pth\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"lane_model_final.pth\")\n",
        "    print(\"Training completed!\")\n",
        "    return model, device"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T12:50:26.905035Z",
          "iopub.execute_input": "2025-04-09T12:50:26.905375Z",
          "iopub.status.idle": "2025-04-09T12:50:26.921169Z",
          "shell.execute_reply.started": "2025-04-09T12:50:26.905351Z",
          "shell.execute_reply": "2025-04-09T12:50:26.920271Z"
        },
        "id": "KPJKp8RuY48h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model,device=train_model()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T12:50:32.178542Z",
          "iopub.execute_input": "2025-04-09T12:50:32.17889Z"
        },
        "id": "DJdH3hHgY48i",
        "outputId": "d1c243e4-0a43-4235-8f66-7ec7c72abdde"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Total annotations in JSON: 3626\nValid images found: 3626\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 1/20: 100%|██████████| 907/907 [06:13<00:00,  2.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [1/20], Loss: 0.0869\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/20: 100%|██████████| 907/907 [06:12<00:00,  2.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [2/20], Loss: 0.0689\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/20: 100%|██████████| 907/907 [06:12<00:00,  2.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [3/20], Loss: 0.0657\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 4/20: 100%|██████████| 907/907 [06:13<00:00,  2.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [4/20], Loss: 0.0633\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 5/20: 100%|██████████| 907/907 [06:14<00:00,  2.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [5/20], Loss: 0.0605\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 6/20: 100%|██████████| 907/907 [06:14<00:00,  2.42it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [6/20], Loss: 0.0580\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 7/20: 100%|██████████| 907/907 [06:13<00:00,  2.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [7/20], Loss: 0.0550\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 8/20: 100%|██████████| 907/907 [06:13<00:00,  2.43it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [8/20], Loss: 0.0518\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 9/20: 100%|██████████| 907/907 [06:11<00:00,  2.44it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch [9/20], Loss: 0.0486\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 10/20:  77%|███████▋  | 699/907 [04:45<01:24,  2.46it/s]",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "NRaGxDS1Y48j"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}